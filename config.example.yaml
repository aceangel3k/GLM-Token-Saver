models:
  local:
    enabled: true
    name: "GLM-4.7-Flash"
    endpoint: "http://your-local-endpoint:41447/v1/chat/completions"
    api_key: "YOUR_API_KEY_HERE"
    model: "unsloth/GLM-4.7-Flash-GGUF"
    context_length: 64000
    max_tokens: 4096
    timeout: 120
    filter_unsupported_params: true  # Filter parameters not supported by llama.cpp
  
  cerebras:
    enabled: true
    name: "Cerebras GLM 4.7"
    endpoint: "https://api.cerebras.ai/v1/chat/completions"
    api_key: "YOUR_CEREBRAS_API_KEY_HERE"
    model: "zai-glm-4.7"
    context_length: 131000
    max_tokens: 4096
    timeout: 120

routing:
  strategy: "adaptive_cerebras"  # Options: smart_routing, smart_speculative, speculative_decoding, adaptive_cerebras, always_local, always_cerebras
    # smart_routing: Use smart routing only (local for simple, cerebras for complex)
    # smart_speculative: Smart routing + speculative decoding for complex tasks
    # speculative_decoding: Always use speculative decoding (draft from local, verify with cerebras)
    # adaptive_cerebras: Always use cerebras until rate limits approached, then switch to local for cooldown
    # always_local: Always use local model
    # always_cerebras: Always use cerebras model
  simple_task_threshold: 1000  # tokens
  complexity_keywords:
    - "code"
    - "debug"
    - "architecture"
    - "optimize"
    - "refactor"
    - "implement"
    - "algorithm"
    - "complex"
    - "difficult"
    - "challenging"
  
speculative_decoding:
  enabled: true
  draft_model: "local"
  verify_model: "cerebras"
  max_draft_tokens: 10
  min_confidence: 0.8
  # Parallel execution settings
  parallel_enabled: false  # Enable parallel speculative decoding
  max_concurrent_drafts: 3  # Number of concurrent drafts to generate
  draft_timeout: 5  # Timeout in seconds for parallel drafts
  # Spillover notification settings
  notification_on_spillover: false  # When False, only log spillovers server-side. When True, send notification to client.
  
cost_tracking:
  budget_limit: 100.0  # USD
  alert_threshold: 80.0
  cerebras_cost_per_1k_tokens: 0.002  # Adjust based on actual pricing
  
server:
  host: "0.0.0.0"
  port: 8000
  log_level: "info"  # Log info, warnings, and errors
  
logging:
  file: "logs/api_server.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5

response_notifications:
  enabled: true  # Set to true to enable model usage notifications in responses
  position: "prepend"  # Options: "prepend", "append", "both"
  #template: "[Model: {model}] "
  template: "\n\n{emoji} Response generated by: {model} ¬∑ Tokens: {tokens} ¬∑ Cost: ${cost}"
  # Available placeholders:
  # - {emoji}: Model-specific emoji (üè† for local, üóÑÔ∏è for cerebras)
  # - {model}: Name of the model used (local/cerebras)
  # - {tokens}: Total tokens used
  # - {cost}: Cost in USD

rate_limit:
  enabled: true  # Enable automatic rate limit tracking and fallback
  # Request limits (based on Cerebras API limits)
  requests_per_minute: 50  # 50 requests per minute
  requests_per_hour: 3000  # 3000 requests per hour
  requests_per_day: 72000  # 72000 requests per day
  # Token limits (based on Cerebras API limits)
  tokens_per_minute: 1000000  # 1M tokens per minute
  tokens_per_hour: 24000000  # 24M tokens per hour
  tokens_per_day: 24000000  # 24M tokens per day
  # Fallback thresholds (percentage of limits to trigger automatic fallback to local)
  request_fallback_threshold: 0.8  # At 80% of request limit, start using local
  token_fallback_threshold: 0.85  # At 85% of token limit, start using local

adaptive_cerebras:
  enabled: true  # Enable adaptive cerebras strategy (respects rate limit headers)
  cooldown_seconds: 60  # How long to use local after hitting rate limits (seconds)
  threshold_percent: 20.0  # Switch to local when tokens remaining < threshold% (e.g., 20%)
