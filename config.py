import logging
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field


class ModelConfig(BaseModel):
    enabled: bool
    name: str
    endpoint: str
    api_key: str = ""
    model: str
    context_length: int
    max_tokens: int
    timeout: int = 120
    filter_unsupported_params: bool = True  # Filter unsupported params for llama.cpp


class RoutingConfig(BaseModel):
    strategy: str = "smart_routing"
    simple_task_threshold: int = 1000
    complexity_keywords: list[str] = Field(default_factory=list)


class SpeculativeDecodingConfig(BaseModel):
    enabled: bool = True
    draft_model: str = "local"
    verify_model: str = "cerebras"
    max_draft_tokens: int = 10
    min_confidence: float = 0.8
    # Parallel execution settings
    parallel_enabled: bool = False
    max_concurrent_drafts: int = 3
    draft_timeout: int = 5
    # Spillover notification settings
    notification_on_spillover: bool = False  # When False, only log spillovers server-side


class CostTrackingConfig(BaseModel):
    budget_limit: float = 100.0
    alert_threshold: float = 80.0
    cerebras_cost_per_1k_tokens: float = 0.002


class ServerConfig(BaseModel):
    host: str = "0.0.0.0"
    port: int = 8000
    log_level: str = "info"


class LoggingConfig(BaseModel):
    file: str = "logs/api_server.log"
    max_bytes: int = 10485760
    backup_count: int = 5


class ResponseNotificationsConfig(BaseModel):
    enabled: bool = False
    position: str = "append"  # Options: "prepend", "append", "both"
    template: str = "\n\n---\nüè† Response generated by: {model} | Tokens: {tokens} | Cost: ${cost}"


class RateLimitConfig(BaseModel):
    enabled: bool = True
    # Request limits
    requests_per_minute: int = 50
    requests_per_hour: int = 3000
    requests_per_day: int = 72000
    # Token limits
    tokens_per_minute: int = 1000000
    tokens_per_hour: int = 24000000
    tokens_per_day: int = 24000000
    # Fallback thresholds (percentage of limits to trigger fallback)
    request_fallback_threshold: float = 0.8  # 80% of limit
    token_fallback_threshold: float = 0.85  # 85% of limit


class CacheConfig(BaseModel):
    enabled: bool = True
    max_size: int = 1000
    default_ttl: int = 3600  # 1 hour in seconds
    cache_by_temperature: bool = True
    cache_by_max_tokens: bool = False


class AdaptiveCerebrasConfig(BaseModel):
    enabled: bool = True
    cooldown_seconds: int = 60  # How long to use local after hitting rate limits
    threshold_percent: float = 20.0  # Switch to local when tokens remaining < threshold%


class Config(BaseModel):
    models: Dict[str, ModelConfig]
    routing: RoutingConfig
    speculative_decoding: SpeculativeDecodingConfig
    cost_tracking: CostTrackingConfig
    server: ServerConfig
    logging: LoggingConfig
    response_notifications: ResponseNotificationsConfig = Field(
        default_factory=ResponseNotificationsConfig
    )
    rate_limit: RateLimitConfig = Field(default_factory=RateLimitConfig)
    cache: CacheConfig = Field(default_factory=CacheConfig)
    adaptive_cerebras: AdaptiveCerebrasConfig = Field(
        default_factory=AdaptiveCerebrasConfig
    )


def load_config(config_path: str = "config.yaml") -> Config:
    """Load configuration from YAML file."""
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_file, "r") as f:
        config_data = yaml.safe_load(f)
    
    return Config(**config_data)


# Global config instance
config: Optional[Config] = None


def get_config() -> Config:
    """Get the global config instance."""
    global config
    if config is None:
        config = load_config()
    return config


def reload_config(config_path: str = "config.yaml") -> None:
    """Reload configuration from YAML file."""
    global config
    config = load_config(config_path)
    logging.info(f"Config reloaded from {config_path}")
