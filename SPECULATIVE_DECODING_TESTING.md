# Speculative Decoding Testing Guide

## Overview

The speculative decoding feature has been fully implemented. It now:
1. Generates a draft from the local model (faster, cheaper)
2. Intelligently decides whether to verify with Cerebras
3. Only spills over to Cerebras when needed (complex or uncertain responses)
4. Merges responses based on similarity scores

## How to Test

### 1. Start the Server

Make sure your server is running with speculative decoding enabled:

```bash
python main.py
```

Check your `config.yaml` has:
```yaml
routing:
  strategy: "speculative_decoding"

speculative_decoding:
  enabled: true
  draft_model: "local"
  verify_model: "cerebras"
  max_draft_tokens: 10
  min_confidence: 0.8
```

### 2. Run the Test Script

```bash
python test_speculative_decoding.py
```

This will run 3 tests:
- **Test 1**: Simple request (likely uses draft only)
- **Test 2**: Complex explanation (will verify with Cerebras)
- **Test 3**: Code generation (will verify with Cerebras)

### 3. Check Logs

Look for these log patterns:

**Draft generation:**
```
=== SPECULATIVE DECODING START ===
Step 1: Generating draft from local model (max 10 tokens)
Draft generated: 8 tokens, 45 chars
```

**Verification skipped:**
```
Draft within acceptable range, skipping verification
Draft accepted without verification (short/simple response)
```

**Verification performed:**
```
Step 2: Verifying draft with Cerebras model
Verification response: 156 tokens, 890 chars
Responses similar (0.85), using draft
```

**Spill over occurred:**
```
Responses differ (0.45), using verified response
Spilled over: True
```

### 4. Manual Testing with curl

**Simple request (should use draft only):**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Say hello"}],
    "max_tokens": 50
  }'
```

**Complex request (should verify with Cerebras):**
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Write a detailed explanation of machine learning"}],
    "max_tokens": 500
  }'
```

## When Does It Spill Over to Cerebras?

The system spills over to Cerebras when:

1. **Draft is incomplete**: When draft tokens reach 90% of max_tokens
2. **Draft is very short**: Less than 50 characters
3. **Uncertain content**: Contains markers like "...", "I think", "maybe"
4. **Low similarity**: Draft and verified response differ significantly

## Response Fields

When using speculative decoding, responses include:

```json
{
  "model_used": "cerebras",
  "speculative_decoding": {
    "draft_tokens": 8,
    "verified_tokens": 156,
    "accepted": false,
    "spilled_over": true,
    "similarity": 0.45
  }
}
```

- `draft_tokens`: Tokens generated by local model
- `verified_tokens`: Tokens from Cerebras verification
- `accepted`: Whether draft was accepted without verification
- `spilled_over`: Whether Cerebras response was used
- `similarity`: Similarity score (0.0 to 1.0)

## Tuning Parameters

Adjust in `config.yaml`:

```yaml
speculative_decoding:
  max_draft_tokens: 10    # Draft token limit
  min_confidence: 0.8     # Similarity threshold for accepting draft
```

- **Lower max_draft_tokens**: More frequent verification, higher quality
- **Higher max_draft_tokens**: Less verification, faster but potentially lower quality
- **Lower min_confidence**: More likely to use draft (saves tokens)
- **Higher min_confidence**: More likely to use Cerebras (better quality)

## Troubleshooting

**No spill over happening:**
- Increase `max_draft_tokens` to allow longer drafts
- Lower `min_confidence` to be more lenient with drafts

**Too much spill over:**
- Decrease `max_draft_tokens` to require more verification
- Increase `min_confidence` to be stricter with draft acceptance

**Cerebras errors:**
- Check your API key in config.yaml
- Check network connectivity
- Review logs for specific error messages

## Performance Monitoring

Check stats endpoint:
```bash
curl http://localhost:8000/stats
```

This shows current configuration and feature status.